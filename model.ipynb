{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhA0ip185dFd",
    "tags": []
   },
   "source": [
    "# Import packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "FhIQXENHYYny"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Vs2m0PLYXOY",
    "tags": []
   },
   "source": [
    "# Define data transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "TsEyyWHw5dFl"
   },
   "outputs": [],
   "source": [
    "# Define the transform \n",
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),             # takes PIL image as input and outputs PIL image\n",
    "        transforms.ToTensor(),              # takes PIL image as input and outputs torch.tensor\n",
    "        transforms.Normalize(mean=[0.4280, 0.4106, 0.3589],  # takes tensor and outputs tensor\n",
    "                             std=[0.2737, 0.2631, 0.2601]),  # see next step for mean and std\n",
    "    ])\n",
    "\n",
    "valid_transform = transforms.Compose([ \n",
    "        transforms.Resize((224,224)),             \n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4280, 0.4106, 0.3589],\n",
    "                             std=[0.2737, 0.2631, 0.2601]), \n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),             \n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4280, 0.4106, 0.3589],\n",
    "                             std=[0.2737, 0.2631, 0.2601]), \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "\n",
    "# url = 'https://drive.google.com/file/d/1eQkUZLWuwnmfWNBjldV6BKgWG5RiR7Ji/view?usp=sharing'\n",
    "# output = 'dataset_folder.zip'\n",
    "# gdown.download(id='1eQkUZLWuwnmfWNBjldV6BKgWG5RiR7Ji', output=output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('/home/jupyter/dataset_folder.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('/home/jupyter/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5Da4YrU5dFp",
    "tags": []
   },
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "4gu9Ul7z5dFr"
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the frames.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.annotations['query'] = None\n",
    "        self.annotations = annotations\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations) \n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        if idx == len(self.annotations):\n",
    "          return blank_data()\n",
    "        \n",
    "        # File reader adjusted for iput files with names 0.jpg, 1.jpg, ..., 200.jpg\n",
    "        image = io.imread(os.path.join(self.root_dir, str(idx) + '.jpg')) \n",
    "        query = np.array(self.annotations[0][idx])\n",
    "        score_annotations = self.annotations['score'][idx] \n",
    "        score_annotations = np.array([score_annotations])\n",
    "\n",
    "        score_annotations = score_annotations.astype('float').reshape(-1, )\n",
    "        \n",
    "        sample = {'image': image, 'query': query, 'score_annotations': score_annotations}\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(Image.fromarray(sample['image']))\n",
    "            sample['score_annotations'] = torch.from_numpy(sample['score_annotations'])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "qKEOU7cMoVc-"
   },
   "outputs": [],
   "source": [
    "def blank_data():\n",
    "  blank_image = torch.FloatTensor(np.loadtxt('./dataset/Frames_for_code/blank_image.txt').reshape((3,224,224)))\n",
    "  blank_query = np.array(['0', '0', '0', '0', '0', '0', '0', '0'])\n",
    "  blank_score = torch.FloatTensor([0])\n",
    "  blank_data = {'image': blank_image, 'query': blank_query, 'score_annotations': blank_score}\n",
    "  return blank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "FWQpEzd6S-yp"
   },
   "outputs": [],
   "source": [
    "def get_dataset(case):\n",
    "\n",
    "    if case == 'train':\n",
    "\n",
    "        with open('./dataset/encodings/train_dataset_with_query_embedding2.csv', 'r') as csvfile:\n",
    "            so = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            so_train_data = []\n",
    "            for row in so:\n",
    "                so_train_data.append(row)\n",
    "\n",
    "        train_scores = pd.read_csv('./dataset/relevance_scores/majority_vote_train.csv', names=['score'])\n",
    "        train_root_dir = './dataset/Frames_for_code/TrainImages'\n",
    "\n",
    "        train_annotations = pd.concat([train_scores, pd.Series(so_train_data)], axis=1, )\n",
    "        train_dataset = dataset(train_annotations, train_root_dir)\n",
    "        transform_train_data = dataset(train_annotations, train_root_dir, train_transform)\n",
    "\n",
    "        return train_dataset, transform_train_data\n",
    "\n",
    "    elif case == 'val':\n",
    "\n",
    "        with open('./dataset/encodings/val_dataset_with_query_embedding.csv', 'r') as csvfile:\n",
    "            so_val = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            so_val_data = []\n",
    "            for row in so_val:\n",
    "                so_val_data.append(row)\n",
    "\n",
    "        val_scores = pd.read_csv('./dataset/relevance_scores/majority_vote_val.csv', names=['score'])\n",
    "        val_root_dir = './dataset/Frames_for_code/ValImages'\n",
    "\n",
    "        val_annotations = pd.concat([val_scores, pd.Series(so_val_data)], axis=1, )\n",
    "        val_dataset = dataset(val_annotations, val_root_dir)\n",
    "        transform_val_data = dataset(val_annotations, val_root_dir, valid_transform)\n",
    "\n",
    "        return val_dataset, transform_val_data\n",
    "\n",
    "    elif case == 'test':\n",
    "\n",
    "        with open('./dataset/encodings/test_dataset_with_query_embedding.csv', 'r') as csvfile:\n",
    "            so_test = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            so_test_data = []\n",
    "            for row in so_test:\n",
    "                so_test_data.append(row)\n",
    "\n",
    "        test_scores = pd.read_csv('./dataset/relevance_scores/majority_vote_test.csv', names=['score'])\n",
    "        test_root_dir = './dataset/Frames_for_code/TestImages'\n",
    "\n",
    "        test_annotations = pd.concat([test_scores, pd.Series(so_test_data)], axis=1, )\n",
    "        test_dataset = dataset(test_annotations, test_root_dir)\n",
    "        transform_test_data = dataset(test_annotations, test_root_dir, test_transform)\n",
    "\n",
    "        return test_dataset, transform_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLQX7SgF5dFu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "fKInLFaa5dFw"
   },
   "outputs": [],
   "source": [
    "def query_embedding(queries, dictionary, max_length):\n",
    "    one_hot = encode_queries_index(queries, dictionary)\n",
    "\n",
    "    one_hot_same_length = []\n",
    "    for query in one_hot:\n",
    "        if len(query) < max_length:\n",
    "            query = [*query, *np.zeros(max_length -len(query))]\n",
    "        else:\n",
    "            query = query[:max_length]\n",
    "            \n",
    "        one_hot_same_length.append(query)\n",
    "        \n",
    "    return torch.Tensor(one_hot_same_length) # return a stack of tensors.\n",
    "\n",
    "def encode_queries_index(queries, dictionary):\n",
    "    encoded_queries = []\n",
    "    for query in queries:\n",
    "        query_words = []\n",
    "        for word in query.split(\" \"):\n",
    "            query_words.append(dictionary.index(word))\n",
    "        encoded_queries.append(query_words)\n",
    "        \n",
    "    return encoded_queries\n",
    "\n",
    "def get_query_embeddings():\n",
    "    # Create the dictionary/ bag of words\n",
    "    d = open(r\"./dataset/titles/sorted_title_words.csv\",\"r\")\n",
    "    d_read = csv.reader(d)\n",
    "    dictionary = []\n",
    "    for row in d_read:\n",
    "        dictionary.append(row[0])\n",
    "\n",
    "    # Create embeddings of the training data titles\n",
    "    q_train = open(r\"./dataset/queries/train_queries.csv\",\"r\")\n",
    "    q_train_read = csv.reader(q_train)\n",
    "    queries_train = []\n",
    "    for row in q_train_read:\n",
    "        queries_train.append(row[0])\n",
    "    queries_train = queries_train[1:]\n",
    "    query_train_embeddings = query_embedding(queries_train, dictionary, 8)\n",
    "\n",
    "    # Create embeddings of the validation data titles\n",
    "    q_val = open(r\"./dataset/queries/val_queries.csv\",\"r\")\n",
    "    q_val_read = csv.reader(q_val)\n",
    "    queries_val = []\n",
    "    for row in q_val_read:\n",
    "        queries_val.append(row[0])\n",
    "    queries_val = queries_val[1:]\n",
    "    query_val_embeddings = query_embedding(queries_val, dictionary, 8)\n",
    "\n",
    "    return query_train_embeddings, query_val_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVe9MjHgTKLq",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Basic Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "2Cwtc6PvTMVG"
   },
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "class BasicModule(torch.nn.Module):\n",
    "   '''\n",
    "   封装了nn.Module，主要提供save和load两个方法\n",
    "   '''\n",
    "   def __init__(self,opt=None):\n",
    "       super(BasicModule,self).__init__()\n",
    "       self.model_name = str(type(self)) # 模型的默认名字\n",
    "\n",
    "   def load(self, path):\n",
    "       '''\n",
    "       可加载指定路径的模型\n",
    "       '''\n",
    "       self.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "\n",
    "   def save(self,name=None):\n",
    "       '''\n",
    "       保存模型，默认使用“模型名字+时间”作为文件名，\n",
    "       如AlexNet_0710_23:57:29.pth\n",
    "       '''\n",
    "\n",
    "       if name is None:\n",
    "           prefix = '/checkpoints/' + self.model_name + '_'\n",
    "           name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
    "       torch.save(self.state_dict(), name)\n",
    "       return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe49BFYw5dFy",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "XtnvFyGr5dFz"
   },
   "outputs": [],
   "source": [
    "class QVSmodel(BasicModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(QVSmodel, self).__init__()\n",
    "        \n",
    "        # self.model = resnet34(pretrained='imagenet')\n",
    "        self.model = models.resnet34(pretrained=True, progress=True) \n",
    "        self.fc1 = torch.nn.Linear(512, 4)\n",
    "        \n",
    "        self.fc_text = torch.nn.Linear(8, 512)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image = self.model.conv1(image)\n",
    "        image = self.model.bn1(image)\n",
    "        image = self.model.relu(image)\n",
    "        image = self.model.maxpool(image)\n",
    "        image = self.model.layer1(image)\n",
    "        image = self.model.layer2(image)\n",
    "        image = self.model.layer3(image)\n",
    "        image = self.model.layer4(image)     \n",
    "    \n",
    "        image = F.avg_pool2d(image, 7)\n",
    "        \n",
    "        # reshape image\n",
    "        image = image.view(image.size(0), -1)\n",
    "    \n",
    "        text = F.relu(self.fc_text(text))\n",
    "        \n",
    "        #Combine image and text by element-wise multiplication. The output dimension is still (1, 512).\n",
    "        t1 = torch.mul(image, text)\n",
    "\n",
    "        #Computes the second fully connected layer\n",
    "        relevance_class_prediction = self.fc1(t1)\n",
    "        \n",
    "        return relevance_class_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hus5Y0Zh5dF0",
    "tags": []
   },
   "source": [
    "# Model training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "GgT9YTt-5dF2"
   },
   "outputs": [],
   "source": [
    "def trainNet(model, train_dataset, val_dataset, n_epochs, learning_rate, betas, eps):\n",
    "    \n",
    "    # For GPU\n",
    "    net = model.cuda()\n",
    "    net = net.float()\n",
    "    net.train()    \n",
    "    \n",
    "    #Get data\n",
    "    data_loader = train_dataset \n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr = learning_rate, betas=betas, eps=eps)\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        count_relevance = 0\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()  \n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i_batch in range(len(data_loader)-1):\n",
    "    \n",
    "            #Get inputs\n",
    "            sample_batched = data_loader[i_batch]\n",
    "            inputs, query, labels = sample_batched['image'], sample_batched['query'], \\\n",
    "            sample_batched['score_annotations']  \n",
    "            \n",
    "            labels_relevance = labels[:]\n",
    "            inputs = inputs.cuda()\n",
    "            labels_relevance = labels_relevance.cuda()\n",
    "            \n",
    "            encoding = torch.from_numpy(query.astype(float))\n",
    "            encoding = encoding.cuda()\n",
    "\n",
    "            outputs = model(inputs[None, ...].float(), encoding.float())\n",
    "            outputs = outputs.cuda()\n",
    "\n",
    "            loss_size = loss(outputs, labels_relevance.long())\n",
    "                        \n",
    "            loss_size.backward()\n",
    "            optimizer.step()  \n",
    "            optimizer.zero_grad()\n",
    "   \n",
    "            running_loss += loss_size.item()\n",
    "    \n",
    "            total_train_loss += loss_size.item()  \n",
    "           \n",
    "            #Compute accuracy\n",
    "            _, preds = torch.max(outputs, dim = 1)\n",
    "            num_correct_relevance = torch.sum(labels_relevance.long() == preds.long())\n",
    "            count_relevance += num_correct_relevance.item()\n",
    "        \n",
    "        \n",
    "            # print(\"Epoch {}, {:d}% \\t train_loss_{}_batch: {:.4f} \\t took: {:.4f}s\".format(\n",
    "            #             epoch+1, int(100 * (i_batch+1) / len(data_loader)), i_batch+1, running_loss, time.time() - start_time))\n",
    "            \n",
    "            if(i_batch % 150 == 0):\n",
    "                print(\"Epoch {}: {:.1f} % training done.\".format(epoch+1,i_batch/len(data_loader)*100))\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        train_acc = (100*float(count_relevance)/(len(data_loader)))\n",
    "        print(\"Training accuracy = {:.4f} for epoch {}\".format(train_acc, epoch +1))\n",
    "        \n",
    "        val_acc = validate(net, val_dataset)\n",
    "        print(\"Validation accuracy = {:.4f} for epoch {}\".format(val_acc, epoch +1))\n",
    "        accuracies.append([train_acc,val_acc]) \n",
    "        \n",
    "        net.train()\n",
    "\n",
    "    print(\"Training finished, took {:.4f}s\".format(time.time() - training_start_time))\n",
    "    return net, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQxhElVVc59f",
    "tags": []
   },
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "cy3IjqXvcfSY"
   },
   "outputs": [],
   "source": [
    "def validate(model, val_loader):       \n",
    "    model.eval() # set to eval mode to avoid batchnorm\n",
    "    with torch.no_grad(): # avoid calculating gradients\n",
    "        correct, total = 0, 0\n",
    "        count_relevance = 0\n",
    "        for i in range(len(val_loader)):\n",
    "            sample_batched = val_loader[i]\n",
    "            images, query, labels = sample_batched['image'], sample_batched['query'], \\\n",
    "            sample_batched['score_annotations']\n",
    "\n",
    "            labels = labels[:]\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            encoding = torch.from_numpy(query.astype(float))\n",
    "            encoding = encoding.cuda()     \n",
    "            outputs = model(images[None, ...].float(), encoding.float())\n",
    "            \n",
    "            #Compute accuracy\n",
    "            _, preds = torch.max(outputs, dim = 1)\n",
    "            num_correct_relevance = torch.sum(labels.long() == preds.long())\n",
    "            count_relevance += num_correct_relevance.item()\n",
    "\n",
    "            if(i % 50 == 0):\n",
    "                print(\"{:.1f} % validation done.\".format(i/len(val_loader)*100))\n",
    "        acc_relevance = (100*float(count_relevance)/(len(val_loader)))\n",
    "    return acc_relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PrDt4WXdCyB"
   },
   "source": [
    "# Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "UAM5vpI-U-sj",
    "outputId": "5dfa5994-cd38-4aff-a18b-4d888529c386"
   },
   "outputs": [],
   "source": [
    "### TO DO: Define model name\n",
    "model_name = 'E_7_L_1e-4_e_1e-8v2' # change this every time\n",
    "\n",
    "### TO DO: Specify training hyperparameters\n",
    "n_epochs = 7\n",
    "learning_rate = 1e-4\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-08\n",
    "\n",
    "### Train model\n",
    "model = QVSmodel()\n",
    "train_dataset, transform_train_data = get_dataset('train')\n",
    "val_dataset, transform_val_data = get_dataset('val')\n",
    "\n",
    "print(validate(model.cuda(), transform_val_data))\n",
    "\n",
    "model, accuracies = trainNet(model, transform_train_data, transform_val_data, n_epochs, learning_rate, betas, eps)\n",
    "\n",
    "### Save model and accuracies\n",
    "path = './models/' + model_name + '.pth'\n",
    "torch.save(model.state_dict(), path)\n",
    "acc_file = open('./models/'+model_name+'.txt', 'w')\n",
    "hypers = 'Learning rate = '+str(learning_rate)+', Betas = '+str(betas)+', Epsilon = '+str(eps)+'\\n'\n",
    "acc_file.write(hypers)\n",
    "for i in range(len(accuracies)):\n",
    "    accs = 'Epoch '+str(i+1)+' : Training accuracy is '+str(accuracies[i][0])+'%, validation accuracy is '+str(accuracies[i][1])+'% \\n'\n",
    "    acc_file.write(accs)\n",
    "acc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, transform_test_data = get_dataset('test')\n",
    "print(validate(model.cuda(), transform_test_data))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GQxhElVVc59f"
   ],
   "name": "Copy of 16_03_model.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
